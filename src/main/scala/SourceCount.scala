import org.apache.spark.sql._

object SourceCount extends App {

  val spark = SparkSession.builder()
    .appName("SourceCount")
    .master("local[*]")
    .config("spark.driver.bindAddress", "localhost")
    .getOrCreate()

  spark.sparkContext.setLogLevel("WARN")

  import spark.implicits._

  val input = Seq(
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001", 2),
    ("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001", "10.0.0.2.54880", 2)).toDF("column0", "column1", "column2", "label")

  input.show()

  input.createTempView("input")
  spark.sql(
    """
      |select input.*,
      |       count(*) over (partition by column2) as count
      |  from input
      |""".stripMargin)
    .show()

}
